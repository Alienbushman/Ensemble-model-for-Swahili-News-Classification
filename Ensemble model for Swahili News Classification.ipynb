{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# reading in the file and only keeping the relavent features\n",
    "df = pd.read_csv('Data/Train.csv')\n",
    "df=df[['content','category']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# splitting the data into a training and test set (used for both the ensemble model and the original dataset)\n",
    "def train_test_split_features(train, test,train_feature, target_feature,vectorise, final_prediction=False):\n",
    "    y_train = train[target_feature]   \n",
    "    X_train = train[train_feature]\n",
    "    X_test = test[train_feature]\n",
    "    if(final_prediction):\n",
    "        y_test = test[target_feature]\n",
    "    else:\n",
    "        y_test = None\n",
    "    feature_names=[]\n",
    "    if(vectorise):\n",
    "        vect = TfidfVectorizer(min_df=5, ngram_range=(1, 4)) # create Count vectorizer.\n",
    "        X_train = vect.fit(X_train).transform(X_train) # transform text_train  into a vector \n",
    "        X_test = vect.transform(X_test) \n",
    "        feature_names = vect.get_feature_names() # to return all words used in vectorizer\n",
    "  \n",
    "    return X_train, X_test, y_train, y_test, feature_names\n",
    "\n",
    "\n",
    "# a method to run the code quickly\n",
    "quickrun=False\n",
    "if(quickrun):\n",
    "    df,_=train_test_split(df, test_size=0.9, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick output of most of the relavent metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "def print_model_performance(target,predicted):\n",
    "    print('outcome of training')\n",
    "    print(classification_report( target,predicted))   #uncomment if you want to see full report \n",
    "    print('test average accuracy ',accuracy_score( target,predicted))\n",
    "    print(confusion_matrix( target,predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "# a list of all models used\n",
    "def all_models():\n",
    "    #Using the recomended classifiers\n",
    "    #https://arxiv.org/abs/1708.05070\n",
    "    GBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\n",
    "    RFC = RandomForestClassifier(n_estimators=500, max_features=0.25, criterion=\"entropy\")\n",
    "    SVM = SVC(C = 0.01, gamma=0.1, kernel=\"poly\", degree=3, coef0=10.0)\n",
    "    ETC = ExtraTreesClassifier(n_estimators=1000, max_features=\"log2\", criterion=\"entropy\")\n",
    "    LR = LogisticRegression(C=1.5,fit_intercept=True)\n",
    "    # Models that were not included in the paper not from SKlearn\n",
    "    XGC = XGBClassifier()\n",
    "    CBC = CatBoostClassifier(silent=True)\n",
    "    light_gb = lgb.LGBMClassifier()\n",
    "    models=[(LR, \"linear_regression\"),(ETC, \"Extra_tree_classifier\"),(SVM, \"support_vector_classifier\"), (RFC, \"random_forest_classifier\"), (GBC, \"gradient_boosted_classifier\"),\n",
    "             (XGC, \"XGBoost\"),(light_gb,\"Light_GBM\"), (CBC, \"catboost_classifier\")]\n",
    "    #this subset was selected due to runtime\n",
    "    models=[(LR, \"linear_regression\"), (GBC, \"gradient_boosted_classifier\"),\n",
    "             (XGC, \"XGBoost\"),(light_gb,\"Light_GBM\")]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# running the dataset with the the model given splitting it into 5 fold cross validation and saving the results\n",
    "def run_features(df, model,predict_probability=False,features='content', vectorise=True):\n",
    "    cv = KFold(n_splits=5, random_state=42, shuffle=False)\n",
    "    full_prediciton=[]\n",
    "    for train_index, test_index in cv.split(df):\n",
    "        df_train, df_test = df.loc[train_index], df.loc[test_index]\n",
    "        X_train, X_test, y_train, y_test, feature_names=train_test_split_features(df_train,df_test,features,'category', vectorise)\n",
    "        model.fit(X_train, y_train)\n",
    "        if (predict_probability==True):\n",
    "            prediction = model.predict_proba(X_test)\n",
    "        else:\n",
    "            prediction = model.predict(X_test)\n",
    "            \n",
    "        full_prediciton.append(prediction)\n",
    "\n",
    "    predictions=[]\n",
    "    for set_of_prediction in full_prediciton:\n",
    "        for predicted in set_of_prediction:\n",
    "            predictions.append(predicted)\n",
    "    return predictions\n",
    "\n",
    "# saving the probabilities of each of the features so that it can be used to train an ensemble model\n",
    "def save_feature_probabilities(df_copy,model_predicted_names,model_name,prediction):\n",
    "    # todo clean up the function a littleA method is a procedure or function in OOPs Concepts. Whereas, a function is a group of reusable code which can be used anywhere in the program. This helps the need for writing the same code again and again. It helps programmers in writing modular codes.01 Jun 2020\n",
    "    pred_1=[]\n",
    "    pred_2=[]\n",
    "    pred_3=[]\n",
    "    pred_4=[]\n",
    "    pred_5=[]\n",
    "    for prediction_1, prediction_2,prediction_3,prediction_4,prediction_5 in prediction:\n",
    "        pred_1.append(prediction_1)\n",
    "        pred_2.append(prediction_2)\n",
    "        pred_3.append(prediction_3)\n",
    "        pred_4.append(prediction_4)\n",
    "        pred_5.append(prediction_5)\n",
    "            \n",
    "    one_predictions=model_name+'_1'\n",
    "    two_predictions=model_name+'_2'\n",
    "    three_predictions=model_name+'_3'\n",
    "    four_predictions=model_name+'_4'\n",
    "    five_predictions=model_name+'_5'\n",
    "    \n",
    "    df_copy[one_predictions]=pred_1\n",
    "    df_copy[two_predictions]=pred_2\n",
    "    df_copy[three_predictions]=pred_3\n",
    "    df_copy[four_predictions]=pred_4\n",
    "    df_copy[five_predictions]=pred_5\n",
    "    \n",
    "    model_predicted_names.append(one_predictions)\n",
    "    model_predicted_names.append(two_predictions)\n",
    "    model_predicted_names.append(three_predictions)\n",
    "    model_predicted_names.append(four_predictions)\n",
    "    model_predicted_names.append(five_predictions)\n",
    "    \n",
    "    pred_1.clear()\n",
    "    pred_2.clear()\n",
    "    pred_3.clear()\n",
    "    pred_4.clear()\n",
    "    pred_5.clear()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_copy, model_predicted_names\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_boosted_classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# the original models either by outputting their prediction probabilities for each of the 5 categories or returning the predicted value\n",
    "model_predicted_names=[]\n",
    "\n",
    "models=all_models()\n",
    "df_copy=df.copy()\n",
    "for model, name in models:\n",
    "    print(name)\n",
    "    predict_probability=True\n",
    "    prediction = run_features(df_copy, model, predict_probability)\n",
    "    \n",
    "    if(predict_probability):\n",
    "        model_name=name+'_prediction'\n",
    "        df_copy, model_predicted_names=save_feature_probabilities(df_copy,model_predicted_names,model_name,prediction)\n",
    "    else:\n",
    "        print_model_performance(df_copy['category'],prediction)\n",
    "    \n",
    "df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using random forrest to create a ensemble model and outputting the results\n",
    "original_ensemble_model = RandomForestClassifier()\n",
    "predict_probability=False\n",
    "predictions=run_features(df_copy,original_ensemble_model,predict_probability=predict_probability, features=model_predicted_names,vectorise=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_performance(df_copy['category'],predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_set=pd.read_csv('Data/Test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the model by making use of the entire training set\n",
    "def running_test_set(df_train, df_test, model,predict_probability=False,features='content', vectorise=True, train_model=True):\n",
    "    X_train, X_test, y_train, y_test, feature_names=train_test_split_features(df_train,df_test,features,'category', vectorise, False)\n",
    "    if(train_model):\n",
    "        model.fit(X_train, y_train)\n",
    "    if (predict_probability==True):\n",
    "        prediction = model.predict_proba(X_test)\n",
    "    else:\n",
    "        prediction = model.predict(X_test)\n",
    "\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predicted_names=[]\n",
    "\n",
    "models=all_models()\n",
    "df_copy=df.copy()\n",
    "df_test_copy=df_test_set.copy()\n",
    "\n",
    "for model, name in models:\n",
    "    print(name)\n",
    "    predict_probability=True\n",
    "    prediction = running_test_set(df_copy,df_test_copy, model, predict_probability)\n",
    "    \n",
    "    if(predict_probability):\n",
    "        model_name=name+'_prediction'\n",
    "        df_test_copy, model_predicted_names=save_feature_probabilities(df_test_copy,model_predicted_names,model_name,prediction)\n",
    "    else:\n",
    "        print_model_performance(df_test_copy['category'],prediction)\n",
    "    \n",
    "df_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predictions predictions and outputting it in the format specified\n",
    "prediction = original_ensemble_model.predict(df_test_copy[model_predicted_names])\n",
    "# if the target is for the best loss and not the best prediction, the prediction values probably perform better\n",
    "prediction_probabilities = original_ensemble_model.predict_proba(df_test_copy[model_predicted_names])\n",
    "df_test_copy['predictions']=prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputting the model\n",
    "output_df=df_test_copy.copy()\n",
    "dummies = pd.get_dummies(output_df['predictions'],  drop_first=False)\n",
    "output_df=pd.concat([output_df, dummies], axis=1)\n",
    "output_df[['swahili_id','Biashara','Burudani','Kimataifa','Kitaifa','michezo']].to_csv(\"submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
